{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1\n",
    "#### Otto Åström and Vili Niemi \n",
    "#### 3.2.2019\n",
    "#### Helsinki Metropolia University of Applied Sciences\n",
    "\n",
    "The purpose of this document is to study making a neural network that predicts the likelyhood of heart diseases based on data taken from the Cleveland Clinic Foundation.  \n",
    "\n",
    "First we'll import the necessary tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll import the processed data from Cleveland using the read_csv function of pandas. The data, however, is still unusable. We add column names, and change the different types of heart disease into just 1 type, in order to predict if a patient is sick or not instead of predicting what illness they might have. It also has six unknown values that are marked with the value '?'. These need to be removed, and the best way to do that is to simply remove the rows they occupy with the drop function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "cl_data = pd.read_csv(url)\n",
    "\n",
    "#Adding column names\n",
    "cl_data.columns=['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
    "\n",
    "#Replacing different types of illness with just one type of ill.\n",
    "cl_data.num.replace([2, 3, 4], [1, 1, 1], inplace=True)\n",
    "\n",
    "#Cleaning up the data of filthy '?' marks\n",
    "cl_data.drop(301, inplace=True)\n",
    "cl_data.drop(286, inplace=True)\n",
    "cl_data.drop(265, inplace=True)\n",
    "cl_data.drop(191, inplace=True)\n",
    "cl_data.drop(165, inplace=True)\n",
    "cl_data.drop(86, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we clean up the data it's time to split it into a training set and a testing set. First we'll split the data into y half, that only contains the value of wether or not someone is sick \"num\", and the X half that contains all the other values but the \"num\". Then we split those two yet again, and create a test and train set for both of them using the train_test_split function. In this function we can control the size of the split using the parameter test_size. Here we have set it to split the data 33%-66%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test and train splits\n",
    "X = cl_data.drop('num', axis=1) \n",
    "y = cl_data['num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally we'll actually start building the neural network. We start by creating a Sequential model, and add to it three Dense type layers.\n",
    "\n",
    "First the input layer where we have chosen to use 12 hidden units, and we are using relu as our activation argument, which means the layer will output an array the size of ('*', 12) and we also inform the model how many values we'll be feeding it using the input_shape parameter.\n",
    "\n",
    "Then we'll add one hidden layer, that like the input layer, uses relu, and outputs an array the size of ('*', 8).\n",
    "\n",
    "Finally the output layer differs from the first two, as it's set to output a single probability of how likely a sample is to have a num value of \"1\" (how likely a patient is to be sick) with the activation argument of sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the constructor\n",
    "model = Sequential()\n",
    "\n",
    "#Add an input layer \n",
    "model.add(Dense(12, activation='relu', input_shape=(13,)))\n",
    "\n",
    "#Add one hidden layer \n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "#Add an output layer \n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll actually use the model we just created. First we'll have to train it with the training data we created prior. \n",
    "\n",
    "We'll start with compiling the model. We configure the compile function with the adam optimizer and add the binary_crossentropy parameter because we are trying to achieve a binary outcome of wether or not the value num is 0 or 1. We also wish to monitor the accuracy of the model during training so we added the accuracy value to the metrics parameter.\n",
    "\n",
    "To fit the model we feed the fit function the training data, and the number of epochs (iterations over the data) and the batch size of 1 sample. The verbose parameter is just there to make the loading look cool. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "198/198 [==============================] - 3s 13ms/step - loss: 1.3324 - acc: 0.5051\n",
      "Epoch 2/10\n",
      "198/198 [==============================] - 1s 4ms/step - loss: 0.8116 - acc: 0.5404\n",
      "Epoch 3/10\n",
      "198/198 [==============================] - 1s 4ms/step - loss: 0.8058 - acc: 0.5707\n",
      "Epoch 4/10\n",
      "198/198 [==============================] - 1s 4ms/step - loss: 0.8057 - acc: 0.5657\n",
      "Epoch 5/10\n",
      "198/198 [==============================] - 1s 4ms/step - loss: 0.7507 - acc: 0.5505\n",
      "Epoch 6/10\n",
      "198/198 [==============================] - 1s 3ms/step - loss: 0.6712 - acc: 0.5960\n",
      "Epoch 7/10\n",
      "198/198 [==============================] - 1s 3ms/step - loss: 0.6586 - acc: 0.6313\n",
      "Epoch 8/10\n",
      "198/198 [==============================] - 1s 3ms/step - loss: 0.6404 - acc: 0.6061\n",
      "Epoch 9/10\n",
      "198/198 [==============================] - 1s 3ms/step - loss: 0.6254 - acc: 0.6717\n",
      "Epoch 10/10\n",
      "198/198 [==============================] - 1s 3ms/step - loss: 0.5958 - acc: 0.6919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb16a0f0588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model we are ready to make predictions with it and evaluate its accuracy. \n",
    "\n",
    "To make a prediction all we need to do is to put the X_test data through the predict function of our model and store that in y_pred. But just doing this doesn't really tell us anything about anything. We'll need to evaluate our model. We can choose to do this in multiple ways, we have chosen to go with the option of just printing the accuracy straight up through the  score value we get from the evaluate function. We could also use the commented out method of printing a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 2ms/step\n",
      "\n",
      " Score: \n",
      " Loss/Accuracy \n",
      " [0.5788441616661695, 0.7142857142857143]\n"
     ]
    }
   ],
   "source": [
    "#Making the prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#Model evaluation\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "print(\"\\n Score: \\n Loss/Accuracy \\n\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. A neural network that makes predictions on heart diseases and evaluates its own accuracy. The accuracy on these settings ranges from 53-82% which is below acceptable limits, however, with more time to play around with them, we believe we could get it to predict with over 70% accuracy on the regular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
